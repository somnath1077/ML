\chapter{Trees, Boosting and Random Forests}

\chapter{Gradient Boosted Regression}

We first consider gradient boosting for classification problems. Suppose that 
we are given a dataset $\{(\vect{x}_i, y_i)_{i = 1}^{n}\}$, where 
$\vect{x}_i \in \R^p$ and $y_i \in \{0, 1\}$. The problem is to find a mapping 
$f \colon \vect{x} \to y$. As in logistic regression, we modify the problem 
slightly and do not work directly with the class labels~$y_i$. Instead, we 
consider the $\log (\text{odds})$ of the event $\pr \{y = 1 \mid \vect{x} \}$. 
Since $- \infty < \log (\text{odds}) < + \infty$, this restatement allows us 
to focus on functions~$F \colon \R^p \to \R$ rather than from 
$\R^p \to \{0, 1\}$. This is helpful because gradient boosting requires 
differentiable loss functions. 

Our next step is to build an appropriate loss function. Let 
$p = \pr \{y = 1 \mid \vect{x} \}$. Then we may write
\[
    \pr \{y \mid \vect{x} \} = p^y \cdot (1 - p)^{1 - y}.
\] 
Consequently, the likelihood of $y_1, \ldots, y_n$ given 
$\vect{x}_1, \ldots, \vect{x}_n$ assuming that the data 
$\{(\vect{x}_i, y_i)_{i = 1}^{n}\}$ are independent is
\[
    \pr \{y_1, \ldots, y_n \mid \vect{x}_1, \ldots, \vect{x}_n \} 
        =   \prod_{i = 1}^{n} p_{i}^{y_i} \cdot (1 - p_i)^{1 - y_i}.
\]
The log-likelihood is $\sum_{i} \left [ y_i \log p_i + (1 - y_i) \log (1 - p_i) \right ]$. 
If we were to fit a model such a logistic regression, we would search for 
those model parameters for which the log-likelihood is a maximum. If we 
were to use a transform of the log-likelihood as a loss function, we would
want to minimize that transform. The easiest transform is the negative 
log-likelihood. Minimizing the negative log-likelihood is equivalent to maximizing
the log-likelihood (which is what we want to do). 
This negative log-likelihood will then be our loss function and this also 
goes by the name of \emph{cross entropy}.
 
We re-write the loss function in terms of $\log (\text{odds})$ because this 
is what our gradient boosting model will output. To simplify the resulting 
expressions, we consider only one data point and omit the data index~$i$. 
We may then write:
\begin{align*}
    - \left [ y \log p + (1 - y) \log (1 - p) \right ] 
        & = - y \log p - (1 - y) \log (1 - p) \\
        & = - y \log p + y \log (1 - p) - \log (1 - p) \\
        & = - y \log \frac{p}{1 - p} - 
                \log \left (1 - \sigma \left ( \log \frac{p}{1 - p} \right ) \right ).
\end{align*}
In the last step, we used the fact that $p = \sigma (p / (1 - p))$, where $\sigma$
is the sigmoid function. This last step can be simplified by expanding out the 
sigmoid function and the loss function can then be written as:
\[
    - y \log \frac{p}{1 - p} + \log (1 + e^{\log \frac{p}{1 - p}}).
\]
Note the positive sign before the logarithm. 

Finally, we note that the gradient 
boosting engine gives us a function $F(\vect{x})$ that represents the 
$\log (\text{odds})$. With this, we can write the loss function more clearly as:
\[
    L(y, F(\vect{x})) = -y F(\vect{x}) + \log (1 + e^{F(\vect{x})}).
\]
This function is differentiable wrt $F(\vect{x})$ and with some manipulation, 
one can show that:
\[
    \frac{\der L}{\der F(\vect{x})} = - y + \sigma (F(\vect{x})). 
\]
Now this has a nice interpretation. The term $\sigma (F(\vect{x}))$ is the 
predicted probability that $y = 1$. If we were to interpret the label 
$y \in \{0, 1\}$ as a probability, then the derivative of the loss function 
wrt $F(\vect{x})$ is the negative of the difference of the actual probability 
and the predicted probability. 

We now consider the gradient boosting algorithm and go through it step by step. 
\begin{description}
    \item[Input.] A dataset $\{(\vect{x}_i, y_i)_{i = 1}^{n}\}$ and a 
        differentiable loss function $L(y, F(\vect{x}))$.
    
    \item[Step~1.] Initialize model with a constant value $F_0(\vect{x})$ such
        that for all $\vect{x}_j$, $1 \leq j \leq n$,
        \[
            F_0(\vect{x}_j) = \argmin_{\gamma} \sum_{i = 1}^{n} L(y_i, \gamma).
        \]   
    \item[Step~2] For $m = 1$ to $M$ (the maximimum number of trees):
        \begin{enumerate}
            \item For each $i \in [1, \ldots, n]$, compute the pseudo residuals:
                \[
                    r_{i m} = 
                            - \left [ 
                                \frac{\partial L(y_i, F(\vect{x}))}{\partial F(\vect{x})} 
                              \right ]_{F(\vect{x}) = F_{m - 1}(\vect{x})}.
                \]
            \item For each $i \in [1, \ldots, n]$, fit a regression tree to the 
                $r_{i m}$ values and create terminal regions $R_{j m}$, where 
                $j = 1, \ldots, J_m$, where $J_m$ is the number of leaves in the 
                $m$th tree.
            \item For each $j \in [1, \ldots, J_m]$, compute an output value for 
                leaf $j$ in tree $m$:
                \[
                    \gamma_{j m} = \argmin_{\gamma} 
                            \sum_{\vect{x}_i \in R_{j m}} L(y_i, F_{m - 1}(\vect{x}_i) + \gamma).
                \]
            \item Update $F_m(\vect{x}) = F_{m - 1}(\vect{x}) + 
                \nu \cdot \sum_{j = 1}^{J_m} \gamma_{j m} I(\vect{x} \in R_{j m})$.
        \end{enumerate}
    \item[Step~3] Return $F_M$.
\end{description}
\chapter{Neural Networks}

