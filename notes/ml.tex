\chapter{Trees, Boosting and Random Forests}

Random forests are built using decision trees. Decision trees are easy to 
build, easy to use and interpret but they are prone to overfitting. 
Overfitting can be minimized using techniques such as \emph{cost complexity 
pruning}. Nevetheless this tendency to overfit causes decision trees to have 
\emph{high variance}. This means that if we fit decision trees to different 
data sets from the same underlying distribution, we are likely see very different
trees. In particular, if we randomly split a given dataset into train and test
and fit a tree to each, the results will likely be quite different. This tendency 
to have a high variance can be reduced by using a general procedure known 
as \emph{bootstrap aggregation} or \emph{bagging}.

The general principle behind bagging is easy enough to see. Let 
$X_1, \ldots, X_n$ be iid random variables from a distribution with mean $\mu$
and variance $\sigma^2$. If we use the sample mean $\bar{X}_n$ as an estimate 
of $\mu$, the variance is $\sigma^2 / n$. As $n$ increases, the variance 
decreases. Even if we drop the assumption of independence and simply assume 
that the variables $X_i$ are identically distributed with a positive pairwise 
correlation~$\rho$, then the variance of the sample mean is:
\begin{align*}
    \var (\bar{X}_n) & = \var \left ( \frac{1}{n} \sum_{i = 1}^{n} X_i \right ) \\
                     & = \frac{1}{n^2} \left ( \sum_{i = 1}^{n} \var (X_i) + 2 \sum_{1 \leq i < j \leq n} \cov (X_i, X_j)\right ) \\
                     & = \frac{1}{n^2} \left ( n \sigma^2 + 2 \cdot {n \choose 2} \rho \sigma^2 \right ) \\
                     & = \frac{\sigma^2}{n} + \frac{n - 1}{n} \rho \sigma^2 \\
                     & = \rho \sigma^2 + (1 - \rho) \frac{\sigma^2}{n}.
\end{align*}
Again, as $n$ increases, the variance of the estimator decreases. Therefore a 
natural way to reduce the variance of a predictor is to use several data sets 
from the population, fit a predictor to each of these data sets and then take 
the average of these predictions. 
\chapter{Gradient Boosted Regression}

Gradient boosting is a technique where a sequence 
$F_0, \ldots, F_M$ of decision trees are constructed where 
each tree $F_m$ in the sequence is fit to the errors of the 
predictor obtained from the trees that precede it. The predictor obtained 
from a sequence of trees $F_0, \ldots, F_{m - 1}$ takes an additive 
form and given an input~$\vect{x}$, the predicted output is:
\begin{equation}
    F_0(\vect{x}) + \nu \cdot \sum_{j = 1}^{m - 1} F_j(\vect{x}),
\end{equation}  
where $\nu$ is the learning rate. This is different from ensemble methods 
such as random forests in that, in the latter, the prediction is the mean predicted 
value over all the learners in the ensemble. 

This technique can be used for both regression and classification. We first look
at regression as the presentation is slightly easier.

\section{Gradient Boosting for Regression}
Consider a regression problem where given $\{ (\vect{x}_i, y_i)\}_{i = 1}^n$, 
where $\vect{x}_i \in \R^p$ and $y_i \in \R$, we wish to find out 
$F \colon \R^p \to \R$ from an appropriate hypothesis class 
such that $F(\vect{x}_i) \approx y_i$ minimizing some 
loss function. The loss function typically used in regression is squared loss
and for a single data point is defined as:
\begin{equation}  
    \label{eqn:sq_loss_single}
    L(y_i, F(\vect{x}_i)) = \frac{1}{2} (y_i - F(\vect{x}_i))^2. 
\end{equation}
The loss for the entire data set is:
\begin{equation}
    \label{eqn:sq_loss_total}
    L(F) = \frac{1}{2} \sum_{i = 1}^{n} (y_i - F(\vect{x}_i))^2.
\end{equation}

Gradient boosting requires loss functions that are differentiable and squared loss
is one such function. Although taking derivates wrt~$F$ 
in Equation~\ref{eqn:sq_loss_total} is trivial, it does provide a useful insight.
\begin{equation}
    \frac{\der L(F)}{\der F} 
        = - \sum_{i = 1}^n (y_i - F(\vect{x}_i))   
\end{equation}
The derivative is the \emph{negative} of the sum of the residuals. This fact 
will be important as we work through the algorithm.

The algorithm itself is presented in Figure~\ref{fig:gbm_algo}. Step~1 of the 
algorithm asks us to initialize the model with a constant value~$F_0$ 
to be computed using:
\begin{equation}
        F_0 
        = \argmin_{\gamma} \sum_{i = 1}^{n} (y_i - \gamma)^2
        = \argmin_{\gamma} \sum_{i = 1}^{n} (y_i^2 - 2 y_i \gamma + \gamma^2)
        \defeq \argmin_{\gamma} g(\gamma)
\end{equation}
Differentiating wrt~$\gamma$ and then setting the resulting expression to $0$
yields:
\[
    \frac{\der g}{\der \gamma} = \sum_{i = 1}^{n} (-2 y_i + 2 \gamma) \set 0
    \Rightarrow \gamma = \frac{\sum_{i = 1}^{n} y_i}{n} = \bar{y}_n.
\]
Thus the inital prediction is simply the mean of the response. This seems 
like the intuitive thing to do. 

Step~2 is where the sequence of trees is constructed. In each iteration, 
the algorithm constructs a new tree based on the residuals of the previous 
predictor. There are $M$ iterations in total resulting in $M$ trees. 
In practice, this is a parameter to algorithm and is determined using techniques 
such as cross-validation. The first step in this sequence is to compute the 
``pseudo residuals'' $r_{i m}$ for each data point using 
Equation~\ref{eqn:pseudo_residuals}. Since our loss function 
is the squared loss, we see that $r_{im}$ evaluates to: 
\[
    r_{i m} = y_i - F_{m - 1}(\vect{x}_i).
\] 
This is exactly the residual as defined in linear regression. In the context 
of gradient boosting, the term pseudo residual is used to remind us that we 
are not working with linear regression.

The second step in the sequence for Step~2 is the construction of a regression 
tree to fit the residuals $r_{i m}$ that were computed. Assume that the $m$th 
tree has leaf nodes $R_{j m}$, where $1 \leq j \leq J_m$. The next step asks 
us to compute the values of these leaf nodes. Differentiating wrt~$\gamma$ 
in Equation~\ref{eqn:leaf_values} and setting to $0$, we obtain that
\begin{equation}
    \gamma_{j m} 
        = \frac{1}{| \{ x_i \in R_{j m} \} |} \sum_{x_i \in R_{j m}} (y_i - F_{m - 1}(x_i)).
\end{equation}
Again this has an intuitive interpretation. The value of the $j$th leaf in the 
$m$th tree is the mean residual value of all the data points that trickle down
to that leaf node. The final step of Step~2 is to update the $m$th predictor 
using the most recently constructed tree.  

Finally, Step~3 simply returns the updated predictor from the very last iteration
in Step~2. To make a prediction at a new point $\vect{x}$, we output:
\[
    F_0(\vect{x}) + \nu \cdot F_1(\vect{x}) + \cdots + \nu \cdot F_M(\vect{x}).
\]

\begin{figure}[th!]
{\small
\begin{description}
    \item[Input.] A dataset $\{(\vect{x}_i, y_i)\}_{i = 1}^{n}$ and a 
        differentiable loss function $L(y, F(\vect{x}))$.
    
    \item[Step~1.] Initialize model with a constant value $F_0(\vect{x})$ such
        that for all $\vect{x}_j$, $1 \leq j \leq n$,
        \begin{equation}
            F_0(\vect{x}_j) = \argmin_{\gamma} \sum_{i = 1}^{n} L(y_i, \gamma).
        \end{equation}   
    \item[Step~2] For $m = 1$ to $M$ (the maximum number of trees):
        \begin{enumerate}
            \item For each $i \in [1, \ldots, n]$, compute the pseudo residuals:
                \begin{equation} \label{eqn:pseudo_residuals}
                    r_{i m} = 
                            - \left [ 
                                \frac{\partial L(y, F(\vect{x}))}{\partial F(\vect{x})} 
                              \right ]_{F(\vect{x}) = F_{m - 1}(\vect{x}), (\vect{x}_i, y_i)}.
                \end{equation}
            \item For each $i \in [1, \ldots, n]$, fit a regression tree to the 
                $r_{i m}$ values and create terminal regions $R_{j m}$, where 
                $j = 1, \ldots, J_m$, where $J_m$ is the number of leaves in the 
                $m$th tree.
            \item For each $j \in [1, \ldots, J_m]$, compute an output value for 
                leaf $j$ in tree $m$:
                \begin{equation}
                    \label{eqn:leaf_values}
                    \gamma_{j m} = \argmin_{\gamma} 
                            \sum_{\vect{x}_i \in R_{j m}} L(y_i, F_{m - 1}(\vect{x}_i) + \gamma).
                \end{equation}
            \item Update $F_m(\vect{x}) = F_{m - 1}(\vect{x}) + 
                \nu \cdot \sum_{j = 1}^{J_m} \gamma_{j m} I(\vect{x} \in R_{j m})$.
        \end{enumerate}
    \item[Step~3] Return $F_M$.
\end{description}
}
 \caption{The Gradient Boosted Algorithm.}
\label{fig:gbm_algo}
\end{figure}


\section{Gradient Boosting for Classification}

We next consider gradient boosting for classification problems. Suppose that 
we are given a dataset $\{(\vect{x}_i, y_i)_{i = 1}^{n}\}$, where 
$\vect{x}_i \in \R^p$ and $y_i \in \{0, 1\}$. The problem is to find a mapping 
$f \colon \vect{x} \to y$. As in logistic regression, we modify the problem 
slightly and do not work directly with the class labels~$y_i$. Instead, we 
consider the $\log (\text{odds})$ of the event $\pr \{y = 1 \mid \vect{x} \}$. 
Since $- \infty < \log (\text{odds}) < + \infty$, this restatement allows us 
to focus on functions~$F \colon \R^p \to \R$ rather than from 
$\R^p \to \{0, 1\}$. This is helpful because gradient boosting requires 
differentiable loss functions. 

Our next step is to build an appropriate loss function. Let 
$p = \pr \{y = 1 \mid \vect{x} \}$. Then we may write
\begin{equation}
    \pr \{y \mid \vect{x} \} = p^y \cdot (1 - p)^{1 - y}.
\end{equation} 
Consequently, the likelihood of $y_1, \ldots, y_n$ given 
$\vect{x}_1, \ldots, \vect{x}_n$ assuming that the data 
$\{(\vect{x}_i, y_i)_{i = 1}^{n}\}$ are independent is
\begin{equation}
    \pr \{y_1, \ldots, y_n \mid \vect{x}_1, \ldots, \vect{x}_n \} 
        =   \prod_{i = 1}^{n} p_{i}^{y_i} \cdot (1 - p_i)^{1 - y_i}.
\end{equation}
The log-likelihood is $\sum_{i} \left [ y_i \log p_i + (1 - y_i) \log (1 - p_i) \right ]$. 
If we were to fit a model such a logistic regression, we would search for 
those model parameters for which the log-likelihood is a maximum. If we 
were to use a transform of the log-likelihood as a loss function, we would
want to minimize that transform. The easiest transform is the negative 
log-likelihood. Minimizing the negative log-likelihood is equivalent to maximizing
the log-likelihood (which is what we want to do). 
This negative log-likelihood will then be our loss function and this also 
goes by the name of \emph{cross entropy}.
 
We re-write the loss function in terms of $\log (\text{odds})$ because this 
is what our gradient boosting model will output. To simplify the resulting 
expressions, we consider only one data point and omit the data index~$i$. 
We may then write:
\begin{align*}
    - \left [ y \log p + (1 - y) \log (1 - p) \right ] 
        & = - y \log p - (1 - y) \log (1 - p) \\
        & = - y \log p + y \log (1 - p) - \log (1 - p) \\
        & = - y \log \frac{p}{1 - p} - 
                \log \left (1 - \sigma \left ( \log \frac{p}{1 - p} \right ) \right ).
\end{align*}
In the last step, we used the fact that $p = \sigma (p / (1 - p))$, where $\sigma$
is the sigmoid function. This last step can be simplified by expanding out the 
sigmoid function and the loss function can then be written as:
\begin{equation}
    - y \log \frac{p}{1 - p} + \log (1 + e^{\log \frac{p}{1 - p}}).
\end{equation}
Note the positive sign before the logarithm. 

Finally, we note that the gradient 
boosting engine gives us a function $F(\vect{x})$ that represents the 
$\log (\text{odds})$. With this, we can write the loss function more clearly as:
\begin{equation}
    L(y, F(\vect{x})) = -y F(\vect{x}) + \log (1 + e^{F(\vect{x})}).
\end{equation}
This function is differentiable wrt $F(\vect{x})$ and with some manipulation, 
one can show that:
\begin{equation}
    \frac{\der L}{\der F(\vect{x})} = - y + \sigma (F(\vect{x})). 
\end{equation}
Now this has a nice interpretation. The term $\sigma (F(\vect{x}))$ is the 
predicted probability that $y = 1$. If we were to interpret the label 
$y \in \{0, 1\}$ as a probability, then the derivative of the loss function 
wrt $F(\vect{x})$ is the negative of the difference of the actual probability 
and the predicted probability. 

We now go through the gradient boosting algorithm in Figure~\ref{fig:gbm_algo} 
step by step, this time for a binary classification problem. 
Step~1 asks us to initialize the model with a constant value~$\gamma$ which 
is the solution to $\argmin_{\gamma} \sum_{i = 1}^{n} L(y_i, \gamma)$. Now, 
\begin{equation}
    \sum_{i =1}^{n} L(y_i, \gamma) 
        = \sum_{i = 1}^n \left ( -y_i \gamma + \log (1 + e^{\gamma}) \right )
        = - \gamma \sum_{i = 1}^{n} y_i + n \cdot \log (1 + e^{\gamma}) 
        \defeq g(\gamma).
\end{equation}
Differentiating this wrt~$\gamma$, we obtain:
\begin{equation}
    \frac{\der g}{\der \gamma} = - \sum_{i = 1}^n y_i + n \cdot \frac{e^{\gamma}}{1 + e^{\gamma}} 
        = - \sum_{i = 1}^n y_i + n \cdot \sigma(\gamma).  
\end{equation}
Setting the right hand expression above to $0$, we obtain:
\begin{equation}
    \gamma = \log \frac{\bar{y}_n}{1 - \bar{y}_n}.
\end{equation}
Thus the initial constant solution is the $\log (\text{odds})$ of the mean 
$\pr \{y = 1 \mid \vect{x} \}$ in the data. Intuitively, this seems like a good 
initial solution to start out with.

Step~2 is where all the trees are constructed. There are $M$ trees in total and 
this number has to be decided beforehand. In practice, this is a parameter to 
algorithm and is determined using techniques such as cross-validation. The first 
step in this sequence asks us to compute ``residuals'' $r_{i m}$ for each data 
point $(\vect{x}_i, y_i)$ and each tree. Computing the residuals amounts 
to computing:
\begin{equation}
    r_{i m} = - \left [ 
                        \frac{\partial L(y, F(\vect{x}))}{\partial F(\vect{x})} 
                \right ]_{F(\vect{x}) = F_{m - 1}(\vect{x}), (\vect{x}_i, y_i)}
            = y_i - \sigma(F_{m - 1}(\vect{x}_i))
\end{equation}  
Note that $F_{m - 1}(\vect{x})$ gives the $\log (\text{odds})$ of the event that 
$y = 1$ given the data~$\vect{x}$. Hence $\sigma(F_{m - 1}(\vect{x}))$ represents 
$\pr \{y = 1 \mid \vect{x} \}$. Consequently, $r_{im}$ is the difference between 
the observed probability and the predicted probability. This looks very much 
like the residuals as defined in linear regression. Thus the name ``pseudo residuals.''

In the next step in the sequence for Step~2 is the construction of a regression 
tree to fit these pseudo residual values. Suppose that the $m$th tree has $J_m$
leaves. The third step determines an appropriate output value of each leaf of 
the tree just constructed. The output value for the $j$th leaf of this tree 
is:
\begin{equation}
    \gamma_{j m} = \argmin_{\gamma} 
                            \sum_{\vect{x}_i \in R_{j m}} L(y_i, F_{m - 1}(\vect{x}_i) + \gamma).
\end{equation} 
This asks us to find that value of~$\gamma$ such that, when added to the 
$\log (\text{odds})$ value of the previous prediction, the sum of the losses
is minimized.
  
We could differentiate the above expression wrt~$\gamma$ but this is potentially very messy. 
Instead, we simplify the loss function by using a second-order Taylor expansion. 
In this context, recall that if $f \colon \R \to \R$ is a function that is infinitely
differentiable at a point~$x$, then
\begin{equation}
    f(x + h) \approx f(x) 
                        + \frac{f^{(1)}(x)}{1!} \cdot h 
                        + \frac{f^{(2)}(x)}{2!} \cdot h^{2} 
                        + \frac{f^{(3)}(x)}{3!} \cdot h^{3}
                        + \cdots.
\end{equation}
Using this, we can write a second-order approximation to our loss function:
\begin{align*}
    L(y_i, F_{m - 1}(\vect{x}_i) + \gamma) 
            & \approx L(y_i, F_{m - 1}(\vect{x}_i)) 
            + \gamma \cdot \frac{\der L(y_i, F_{m - 1}(\vect{x}_i))}{\der F_{m-1}} 
            + \frac{\gamma^2}{2} \cdot \frac{\der^2 L(y_i, F_{m - 1}(\vect{x}_i))}{\der F_{m-1}^2} \\
            & = L(y_i, F_{m - 1}(\vect{x}_i)) 
            + \gamma \cdot (-y_i + \sigma(F_{m - 1}(\vect{x}_i)) 
            + \frac{\gamma^2}{2} \cdot \sigma(F_{m - 1}(\vect{x}_i)) (1 - \sigma(F_{m - 1}(\vect{x}_i))).
\end{align*}
Differentiate the right-hand side wrt~$\gamma$ to obtain:
\begin{equation}
    -y_i + \sigma(F_{m - 1}(\vect{x}_i) + \gamma \cdot \sigma(F_{m - 1}(\vect{x}_i)) (1 - \sigma(F_{m - 1}(\vect{x}_i))).
\end{equation}
Set this to $0$ and solve for $\gamma$:
\begin{equation}
    \gamma = \frac{y_i - \sigma(F_{m - 1}(\vect{x}_i)}{\sigma(F_{m - 1}(\vect{x}_i)) (1 - \sigma(F_{m - 1}(\vect{x}_i)))}
           = \frac{y_i - p_i}{p_i (1 - p_i)}.
\end{equation}

This is the expression for just a single data point. Taking into account all data points, 
we obtain:
\begin{equation}
\label{eqn:leaf_value}
    \gamma = \frac{\sum_{i = 1}^{n} (y_i - \sigma(F_{m - 1}(\vect{x}_i))}{\sum_{i = 1}^{n}\sigma(F_{m - 1}(\vect{x}_i)) (1 - \sigma(F_{m - 1}(\vect{x}_i)))}
           = \frac{\sum_i (y_i - p_i)}{\sum_i p_i (1 - p_i)}.
\end{equation}
%The numerator of Equation~\ref{eqn:leaf_value} is the sum of pseudo residuals. 
%The denominator
We can now evaluate the value~$\gamma_{j m}$ of each leaf node of the $m$th tree.
The fourth and final step of Step~2 is to update the prediction function 
$F_{m - 1}$ using the values of the leaf nodes of the tree just constructed.
\[
    F_{m}(\vect{x}) = F_{m - 1}(\vect{x}) + 
            \nu \cdot \sum_{j = 1}^{J_m} \gamma_{j m} I(\vect{x} \in R_{j m}).
\] 
Step~3 simply outputs the function~$F_{M}$ obtained after the updation in the 
$M$th round. To make a prediction at a new point $\vect{x}$, we first compute
the $\log (\text{odds})$:
\[
    F_0(\vect{x}) + \nu \cdot F_1(\vect{x}) + \cdots + \nu \cdot F_M(\vect{x}).
\]
Using this, we compute the probability $\sigma(\log (\text{odds}))$ that the 
corresponding label is a $1$. If this probability exceeds a threshold, which 
is usually set to $0.5$, then we output a $1$; otherwise, we output a $0$.  

\chapter{Neural Networks}

