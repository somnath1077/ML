\documentclass[11pt,a4]{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{url}
\usepackage[left=2cm,right=2cm,top=3cm,bottom=2cm]{geometry}

\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\sd}{\sigma}
\DeclareMathOperator{\pr}{P}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\var}{Var}
% For special timeseries
\DeclareMathOperator{\IID}{IID}
\DeclareMathOperator{\WN}{WN}
% for the differential operator
\DeclareMathOperator{\der}{d}

\newcommand{\ts}[3]{\ensuremath{\left \{ #1 \right \}_{#2}^{#3}}}
\newcommand{\eqd}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\iid}{\ensuremath{\stackrel{\text{iid}}{\sim}}}

\theoremstyle{definition}
\newtheorem{example}{Example}

\title{ML Preparation Notes}

\begin{document}
\maketitle

\section{Basics}

The \emph{correlation} between two sets of data is a measure of the strength of
the relationship between them. In particular, Pearson's correlation coefficient
is a measure of linear relationship between two sets of data. Let $X$ and $Y$
be two random variables. Then Pearson's correlation coefficient $\rho(X, Y)$ is
defined as:
\begin{equation}
    \rho(X, Y) = \frac{\cov(X, Y)}{\sd_X \sd_Y} 
\end{equation}  
Two important facts about the Pearson's correlation coefficient~\cite{CasBer}:
\begin{enumerate}
	\item $-1 \leq \rho(X, Y) \leq 1$
	\item $|\rho(X, Y)| = 1$ iff there exists $a \neq 0$ and $b$ such that 
	$Y = aX + b$.
\end{enumerate}


\section{Timeseries}

The material in the section follows~\cite{BroDav}. A \emph{time series} is a
sequence of random variables $\{X_1, X_2, \ldots \}$. A complete model of such
a time series would require specifying the joint probability distributions of
all random vectors $(X_1, \ldots, X_n)'$ for all $n \geq 1$. In practice, this
might be impossible to do unless the time series is generated by some simple
well-understood mechanism. Instead one typically specifies the first- and
second-order moments of the joint distibutions, that is, $\E[X_t]$ and $\E[X_t
X_{t + h}]$ for all $t \geq 1$ and for all $h \geq 0$. 

The next important concepts are those of stationarity and the auto-correlation
function. Roughly speaking, a time series $\ts{X_t}{t = -\infty}{\infty}$ is
stationary if its ``statistical properties'' are similar to those of the
time-shifted series $\ts{X_{t + h}}{t = -\infty}{\infty}$ for every integer
``lag''~$h$. By statistical properties, we mean the first- and second-order
moments of $\ts{X_t}{}{}$. 

Formally, a time series $\ts{X_t}{}{}$ is weakly stationary if 
\begin{enumerate}
	\item $\E[X_t]$ is independent of $t$
	\item $\cov(X_t, X_{t + h})$ is independent of $t$ for every fixed lag~$h$.
\end{enumerate} 
In contrast, a time series $\ts{X_t}{}{}$ is strictly stationary if the random
vectors $(X_{t_1}, \ldots, X_{t_n})'$ and $(X_{t_1 + h}, \ldots, X_{t_n + h})'$
have the same joint distributions for all sets of indices $\{t_1, \ldots,
t_n\}$, for all $h \geq 0$ and all $n \geq 1$. This is written as:
\[
(X_{t_1}, \ldots, X_{t_n})' \eqd (X_{t_1 + h}, \ldots, X_{t_n + h})'
\] 

Strict stationarity implies the following:
\begin{enumerate}
\item The random variables $X_t$ are identically distributed.
	
	\item Pairs of random variables $(X_t, X_{t + h})$ have the same
distribution as $(X_1, X_{1 + h})$ (set $n = 2$).
	
	\item Strict stationarity implies weak stationarity: $\E[X_t] =
\E[X_1]$ and $\cov(X_t, X_{t + h}) = \cov(X_1, X_{1 + h})$ for all $t \geq 1$
and all $h \geq 0$. Both terms are independent of $t$.
	
	\item Weak stationarity does \emph{not} imply strong stationarity. We
show this by an example. Let $Z_i \iid N(0, 1)$ for all $i$. Define $X_t$ as:
\[ X_t = \left \{ \begin{array}{ll} Z_t   & \text{ if $t$ is even}\\ 
                                    2 Z_t & \text{ if $t$ is odd}.   
                  \end{array}	
         \right . 
\] 
Then $\E[X_t] = 0$ for all $t$. Also, 
\[
    \cov(X_t, X_{t + h}) = \left \{ 
                                \begin{array}{ll}
                                    0 & \text{ if } h > 0 \\
                                    1 & \text{ if } h = 0.
                                \end{array}
                           \right . 
\]
This follows from the assumed independence of $X_t$ and $X_{t + h}$ when $h > 0$. 
However, $X_t$s do not have the same distribution, a requirement of 
strict stationarity.
\end{enumerate}

The autocovariance function of a stationary time series $\{ X_t \}$ at lag~$h$
is defined as $\cov(X_{t + h}, X_{t}) = \cov(X_{h}, X_{0})$. The autocorrelation
of $\{ X_t \}$ at lag~$h$ is defined as 
\[
	\frac{ \cov(X_{t + h}, X_t) }{ \var(X_t) } = 
	\frac{ \cov( X_h, X_0 )}{ \var(X_0) }.
\]   

\begin{example}[iid Noise]
Suppose $\{ X_t \}$ is iid noise with zero mean and 
$\E [X_t^2] = \sigma^2 < \infty$. Then $\E [X_t] = 0$ for all $t$ and for every
fixed lag $h$ and all $t$:
\[\cov(X_t, X_{t + h}) = \left \{ 
                                \begin{array}{ll}
                                    0        & \text{ if } h > 0 \\
                                    \sigma^2 & \text{ if } h = 0.
                                \end{array}
                          \right . 
\]
Thus iid noise is stationary. We denote such a series as 
$\{ X_t \} \sim \IID (0, \sigma^2)$.
\end{example}

\begin{example}[White Noise]
A sequence $\{ X_t \}$ of \emph{uncorrelated} random variables with zero mean 
and finite second moment $\sigma^2$ is called \emph{white noise}. Clearly, the 
covariance function at lag $h$ is the same as that of iid noise and, as such, 
white noise is stationary. We denote such a series as
$\{ X_t \} \sim \WN (0, \sigma^2)$. Unlike iid noise, the 
components of white noise need not be independent (recall that independence 
implies zero correlation but not the other way around). In particular, every
$\IID (0, \sigma^2)$ sequence is a $\WN (0, \sigma^2)$ sequence 
but not the other way around. 
\end{example}

\begin{example}[Random Walk]
 A \emph{random walk} $\{ S_t \}$ is a sequence obtained by cumulatively summing 
iid random variables. A random walk with zero mean is obtained by defining 
$S_0 = 0$ and 
\[
    S_t = X_1 + \cdots + X_t
\]
for all $t > 0$, where $\{ X_t \} \sim \IID(0, \sigma^2)$. In this case, 
$\E [S_t] = 0$ and $\E [S_t^2] = t \sigma^2 < \infty$ for all $t$. For all lags 
$h \geq 0$ and all $t$,
\begin{align*}
    \cov (S_t, S_{t + h}) 
        & = \cov (S_t, S_t + X_{t + 1} + \cdots + X_{t + h}) \\
        & = \cov (S_t, S_t) + \cov (S_t, X_{t + 1}) + \cdots + \cov (S_t, X_{t + h}) \\
        & = t \sigma^2.  
\end{align*}
The last equality follows since $\cov (S_t, X_{t + i}) = 0$ for all $i \geq 1$.
Hence $\cov (S_t, S_{t + h})$ depends on $t$ and $\{ S_t \}$ is not stationary.
\end{example}

\section{Bayesian Statistics and MCMC}
This section is based on Chapters~12--15 from~\cite{Lam}. Bayes' rule gives us a
 recipe for calculating the posterior probability density. 
\begin{equation}
	\pr (\Theta \mid \text{data}) = 
	\frac{\pr (\text{data} \mid \Theta) \cdot \pr (\Theta)}{\pr (\text{data})}.
\end{equation}
Consider a case in which we have a sample of $N$ data points $x_1, \ldots, x_N$. 
We assume that the likelihood is a Poisson distribution with mean $\lambda$ and 
that the prior for $\lambda$ is a log-normal$(1, 1)$ distribution. To calculate 
the probability of the data $\pr (\text{data})$, we must evaluate the integral:
\begin{equation}
	\pr (\text{data}) = \int_{0}^{\infty} 
		\prod_{i = 1}^N \frac{\lambda^{x_i} e^{- \lambda}}{x_i!} \cdot 
		\frac{1}{\sqrt{2 \pi} \lambda} e^{- \frac{1}{2} (\log \lambda - 1)^2} 
		d\lambda.
\end{equation}
While this integral is not too difficult, it explains the problem of calculating 
posteriors analytically. As the number of parameters (the length of $\Theta$) 
increases, calculating the probability of the data requires evaluating integrals 
in higher dimensional spaces. This is why we use alternative methods to derive 
approximate versions of the posterior.

\section{Trees, Boosting and Random Forests}

\section{Neural Networks}

\begin{thebibliography}{99}

\bibitem{BroDav} Peter J.~Brockwell, Richard A.~Davis. 
\emph{Introduction to Time Series and Forecasting}, Third Edition, Springer, 
2016. 

\bibitem{CasBer} George Casella, Roger L.~Berger. \emph{Statistical Inference}, 
Second Edition, Duxbury Advanced Series, 2001.

\bibitem{Hyn} Rob J.~Hyndman and George Athanasopoulos. \emph{Forecasting: 
Principles and Practice}, Second Edition. Online book at: 
\url{https://otexts.com/fpp2/}

\bibitem{Lam} Ben Lambert. \emph{A Student's Guide to Bayesian Statistics}, 
Sage Publications, 2018. 

\end{thebibliography}
\end{document}
