\documentclass[11pt,a4]{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{url}
\usepackage[left=2cm,right=2cm,top=3cm,bottom=2cm]{geometry}

\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\sd}{\sigma}
\DeclareMathOperator{\pr}{P}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\var}{Var}
% For special timeseries
\DeclareMathOperator{\IID}{IID}
\DeclareMathOperator{\WN}{WN}
% for the differential operator
\DeclareMathOperator{\der}{d}

\newcommand{\ts}[3]{\ensuremath{\left \{ #1 \right \}_{#2}^{#3}}}
\newcommand{\eqd}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\set}{\ensuremath{\stackrel{\text{set}}{=}}}
\newcommand{\iid}{\ensuremath{\stackrel{\text{iid}}{\sim}}}

\theoremstyle{definition}
\newtheorem{example}{Example}

\title{ML Preparation Notes}

\begin{document}
\maketitle

\section{Basic Statistics}
In this section, we review some basic concepts from Statistics. The material 
in this section is based on~\cite{DegSch}.

\subsection{Covariance and Correlation}

The \emph{correlation} between two sets of data is a measure of the strength of
the relationship between them. In particular, Pearson's correlation coefficient
is a measure of linear relationship between two sets of data. Let $X$ and $Y$
be two random variables. Then Pearson's correlation coefficient $\rho(X, Y)$ is
defined as:
\begin{equation}
    \rho(X, Y) = \frac{\cov(X, Y)}{\sd_X \sd_Y} 
\end{equation}  
Two important facts about the Pearson's correlation coefficient~\cite{CasBer}:
\begin{enumerate}
	\item $-1 \leq \rho(X, Y) \leq 1$
	\item $|\rho(X, Y)| = 1$ iff there exists $a \neq 0$ and $b$ such that 
	$Y = aX + b$.
\end{enumerate}

\subsection{Maximum Likelihood Estimation}
Let the random variables $X_1, \ldots, X_n$ form a random sample from a 
distribution with pdf~$f(x \mid \theta)$. Recall that this means that 
$X_i \iid f( \cdot \mid \theta)$ for all~$1 \leq i \leq n$. 
Let $f_n(\vec{x} \mid \theta)$ denote the value of the joint pdf of 
the random vector $(X_1, \ldots, X_n)'$ at the point 
$\vec{x} = (x_1, \ldots, x_n)'$. The \emph{likelihood function} is the joint pdf 
of the observations of a random sample viewed as a function of $\theta$ for 
a given set of values of the sample. The maximum likelihood estimate of 
$\theta$ is that value of $\theta$ for which $f_n (\vec{x} \mid \theta)$ is 
maximized. 
\begin{example}
Suppose that $X_1, \ldots, X_n$ form a random sample from a distribution with 
pdf $f(x \mid \theta)$ defined as follows:
\[
    f(x \mid \theta) = 
        \left \{
            \begin{array}{ll}
                \theta x^{\theta - 1} & \text{ for } 0 < x < 1 \\
                0                     & \text{ otherwise}.
            \end{array} 
        \right .
\]
Let us assume that $\theta > 0$. The joint distribution of the vector 
$(X_1, \ldots, X_n)'$ at the point $\vec{x} = (x_1, \ldots, x_n)'$ is 
\[
    f_n (\vec{x} \mid \theta ) = \prod_{i = 1}^n \theta x_i^{\theta - 1},
\]
where we assume that $0 < x_i < 1$ for all $i$. Taking logs, we obtain that
$\log f_n (\vec{x} \mid \theta) = n \log \theta + (\theta - 1) \sum_i \log x_i$. 
Take the derivative wrt~$\theta$ and set to $0$:
\[
    \frac{\partial \log f_n (\vec{x} \mid \theta)}{\partial \theta} = 
        \frac{n}{\theta} + \sum_i \log x_i \set 0
\]
to obtain that $\theta = n / \sum_i \log \frac{1}{x_i}$.
\end{example}   

\begin{example}
Suppose that $X_1, \ldots, X_n$ form a random sample from a distribution with 
pdf $f(x \mid \theta)$ defined as follows:
\[
    f(x \mid \theta) = \frac{1}{2} e^{- |x - \theta|} 
        \quad \text{ for } -\infty < x < \infty.
\]
Suppose that $\theta$ is unknown and that $-\infty < \theta < \infty$. 
In this case, the joint probability distribution is easily seen to be:
\[
    f_n(\vec{x} \mid \theta) = \frac{1}{2^n} e^{- \sum_i |x_i - \theta|}.
\]
Take logs to obtain: 
$\log f_n (\vec{x} \mid \theta) = \log \frac{1}{2^n} - \sum_i |x_i - \theta|$. 
Maximizing $f_n$ is equivalent to minimizing $\sum_i |x_i - \theta|$. This 
is equivalent to obtaining a point on the real line that minimizes the sum 
of the distances to the points $x_1, \ldots, x_n$.
\end{example}

\section{Timeseries}

The material in the section follows~\cite{BroDav}. A \emph{time series} is a
sequence of random variables $\{X_1, X_2, \ldots \}$. A complete model of such
a time series would require specifying the joint probability distributions of
all random vectors $(X_1, \ldots, X_n)'$ for all $n \geq 1$. In practice, this
might be impossible to do unless the time series is generated by some simple
well-understood mechanism. Instead one typically specifies the first- and
second-order moments of the joint distibutions, that is, $\E[X_t]$ and $\E[X_t
X_{t + h}]$ for all $t \geq 1$ and for all $h \geq 0$. 

The next important concepts are those of stationarity and the auto-correlation
function. Roughly speaking, a time series $\ts{X_t}{t = -\infty}{\infty}$ is
stationary if its ``statistical properties'' are similar to those of the
time-shifted series $\ts{X_{t + h}}{t = -\infty}{\infty}$ for every integer
``lag''~$h$. By statistical properties, we mean the first- and second-order
moments of $\ts{X_t}{}{}$. 

Formally, a time series $\ts{X_t}{}{}$ is weakly stationary if 
\begin{enumerate}
	\item $\E[X_t]$ is independent of $t$
	\item $\cov(X_t, X_{t + h})$ is independent of $t$ for every fixed lag~$h$.
\end{enumerate} 
In contrast, a time series $\ts{X_t}{}{}$ is strictly stationary if the random
vectors $(X_{t_1}, \ldots, X_{t_n})'$ and $(X_{t_1 + h}, \ldots, X_{t_n + h})'$
have the same joint distributions for all sets of indices $\{t_1, \ldots,
t_n\}$, for all $h \geq 0$ and all $n \geq 1$. This is written as:
\[
(X_{t_1}, \ldots, X_{t_n})' \eqd (X_{t_1 + h}, \ldots, X_{t_n + h})'
\] 

Strict stationarity implies the following:
\begin{enumerate}
\item The random variables $X_t$ are identically distributed.
	
	\item Pairs of random variables $(X_t, X_{t + h})$ have the same
distribution as $(X_1, X_{1 + h})$ (set $n = 2$).
	
	\item Strict stationarity implies weak stationarity: $\E[X_t] =
\E[X_1]$ and $\cov(X_t, X_{t + h}) = \cov(X_1, X_{1 + h})$ for all $t \geq 1$
and all $h \geq 0$. Both terms are independent of $t$.
	
	\item Weak stationarity does \emph{not} imply strong stationarity. We
show this by an example. Let $Z_i \iid N(0, 1)$ for all $i$. Define $X_t$ as:
\[ X_t = \left \{ \begin{array}{ll} Z_t   & \text{ if $t$ is even}\\ 
                                    2 Z_t & \text{ if $t$ is odd}.   
                  \end{array}	
         \right . 
\] 
Then $\E[X_t] = 0$ for all $t$. Also, 
\[
    \cov(X_t, X_{t + h}) = \left \{ 
                                \begin{array}{ll}
                                    0 & \text{ if } h > 0 \\
                                    1 & \text{ if } h = 0.
                                \end{array}
                           \right . 
\]
This follows from the assumed independence of $X_t$ and $X_{t + h}$ when $h > 0$. 
However, $X_t$s do not have the same distribution, a requirement of 
strict stationarity.
\end{enumerate}

The autocovariance function of a stationary time series $\{ X_t \}$ at lag~$h$
is defined as $\cov(X_{t + h}, X_{t}) = \cov(X_{h}, X_{0})$. The autocorrelation
of $\{ X_t \}$ at lag~$h$ is defined as 
\[
	\frac{ \cov(X_{t + h}, X_t) }{ \var(X_t) } = 
	\frac{ \cov( X_h, X_0 )}{ \var(X_0) }.
\]   

\begin{example}[iid Noise]
Suppose $\{ X_t \}$ is iid noise with zero mean and 
$\E [X_t^2] = \sigma^2 < \infty$. Then $\E [X_t] = 0$ for all $t$ and for every
fixed lag $h$ and all $t$:
\[\cov(X_t, X_{t + h}) = \left \{ 
                                \begin{array}{ll}
                                    0        & \text{ if } h > 0 \\
                                    \sigma^2 & \text{ if } h = 0.
                                \end{array}
                          \right . 
\]
Thus iid noise is stationary. We denote such a series as 
$\{ X_t \} \sim \IID (0, \sigma^2)$.
\end{example}

\begin{example}[White Noise]
A sequence $\{ X_t \}$ of \emph{uncorrelated} random variables with zero mean 
and finite second moment $\sigma^2$ is called \emph{white noise}. Clearly, the 
covariance function at lag $h$ is the same as that of iid noise and, as such, 
white noise is stationary. We denote such a series as
$\{ X_t \} \sim \WN (0, \sigma^2)$. Unlike iid noise, the 
components of white noise need not be independent (recall that independence 
implies zero correlation but not the other way around). In particular, every
$\IID (0, \sigma^2)$ sequence is a $\WN (0, \sigma^2)$ sequence 
but not the other way around. 
\end{example}

\begin{example}[Random Walk]
 A \emph{random walk} $\{ S_t \}$ is a sequence obtained by cumulatively summing 
iid random variables. A random walk with zero mean is obtained by defining 
$S_0 = 0$ and 
\[
    S_t = X_1 + \cdots + X_t
\]
for all $t > 0$, where $\{ X_t \} \sim \IID(0, \sigma^2)$. In this case, 
$\E [S_t] = 0$ and $\E [S_t^2] = t \sigma^2 < \infty$ for all $t$. For all lags 
$h \geq 0$ and all $t$,
\begin{align*}
    \cov (S_t, S_{t + h}) 
        & = \cov (S_t, S_t + X_{t + 1} + \cdots + X_{t + h}) \\
        & = \cov (S_t, S_t) + \cov (S_t, X_{t + 1}) + \cdots + \cov (S_t, X_{t + h}) \\
        & = t \sigma^2.  
\end{align*}
The last equality follows since $\cov (S_t, X_{t + i}) = 0$ for all $i \geq 1$.
Hence $\cov (S_t, S_{t + h})$ depends on $t$ and $\{ S_t \}$ is not stationary.
\end{example}

\begin{example}[First-Order Moving Average]
Consider the series defined by
\begin{equation}
    X_t = Z_t + \theta Z_{t - 1}, \quad t = 0, \pm 1, \pm 2, \ldots,
\end{equation}
where $Z_t \sim \WN(0, \sigma^2)$ and $\theta$ is a real-valued constant. Now 
$\E [X_t] = 0$ and 
\begin{align*}
    \E [X_t^2] & = \E [ (Z_t + \theta Z_{t - 1})^2 ] \\
               & = \E [ Z_t^2 + 2 \theta Z_t Z_{t - 1} + \theta^2 Z_{t - 1}^2] \\
               & = \sigma^2 + 2 \theta \E [Z_t Z_{t - 1}] +  \theta^2 \sigma^2 \\
               & = (1 + \theta^2) \sigma^2.
\end{align*}
The last equality follows since, $Z_t$ and $Z_{t - 1}$ being uncorrelated, 
satisfy $\cov (Z_t, Z_{t - 1}) = 0$. Recall that 
$\cov (Z_t, Z_{t - 1}) = \E [Z_t Z_{t - 1}] - \E [Z_t] \E [Z_{t - 1}]$ 
and that being uncorrelated is sufficient for the expectation of the product 
of two random variables to be equal to the product of their expectations. One 
can easily verify that for all $t$
\begin{equation}
    \cov (X_t, X_{t + h}) = 
        \left \{ 
            \begin{array}{ll}
                (1 + \theta^2) \sigma^2 & \text{ if } h = 0 \\
                \theta \sigma^2         & \text{ if } h = \pm 1 \\
                0                       & \text{ if } |h| \geq 2.
            \end{array}        
        \right . 
\end{equation} 
Thus the conditions of weak stationarity hold and the sequence $\{ X_t \}$ 
is weakly stationary. The autocorrelation function is given by:
\begin{equation}
    \rho (h) = \frac{\cov (X_0, X_{h})}{\var(X_0)} = 
        \left \{ 
            \begin{array}{ll}
                1                             & \text{ if } h = 0 \\
                \frac{\theta}{1 + \theta^2}   & \text{ if } h = \pm 1 \\
                0                             & \text{ if } |h| \geq 2.
            \end{array}        
        \right . 
\end{equation} 
\end{example}

\begin{example}[First-Order Autoregression]
Let $\{ X_t \}$ be a stationary series satisfying the equation:
\begin{equation}
    X_t = \phi X_{t - 1} + Z_t \quad t = 0, \pm 1, \pm 2, \ldots,
\end{equation}
where $\{ Z_t \} \sim \WN(0, \sigma^2)$, $|\phi| < 1$ and $Z_t$ is uncorrelated 
with $X_s$ for all $s < t$. As can be seen, $\E [X_t] = 0$ and for $h > 0$:
\begin{align*}
\cov (X_{t + h}, X_t) 
    & = \cov (\phi X_{t + h - 1} + Z_{t + h}, X_t) \\
    & = \phi \cov (X_{t + h - 1}, X_t) + \cov (Z_{t + h}, X_t) \\
    & = \phi \cov (X_{t + h - 1}, X_t).
\end{align*}
From this, one can show that $\cov (X_{t + h}, X_t) = \phi^h \cov (X_t, X_t)$. 
By assumption, $\{ X_t \}$ is stationary and hence 
$\cov (X_{t + h}, X_t) = \phi^h \cov (X_0, X_0)$
Next suppose that $h < 0$. Let $s = t + h = t - |h|$ so that $t = s + |h|$. 
Then $\cov (X_{t + h}, X_t) = \cov (X_s, X_{s + |h|}) = 
\phi^{|h|} \cov (X_s, X_s) = \phi^{|h|} \cov (X_0, X_0)$. 
The autocorrelation function at lag $h$ is 
\[
    \rho(h) = \frac{ \phi^{|h|}  \cov (X_0, X_0) }{ \cov (X_0, X_0)} = \phi^{|h|}.
\]

One can also obtain a closed-form expression for $\cov (X_0, X_0)$. Since 
$\cov (X_0, X_0) = \cov (X_t, X_t)$ and 
\[
    \cov (X_t, X_t) = \cov (\phi X_{t - 1} + Z_t, \phi X_{t - 1} + Z_t) 
    = \phi^2 \cov (X_0, X_0) + \cov (Z_t, Z_t), 
\]
we obtain that $\cov (X_0, X_0) = \sigma^2 / (1 - \phi^2)$.
\end{example}

\section{Bayesian Statistics and MCMC}
This section is based on Chapters~12--15 from~\cite{Lam}. Bayes' rule gives us a
 recipe for calculating the posterior probability density. 
\begin{equation}
	\pr (\Theta \mid \text{data}) = 
	\frac{\pr (\text{data} \mid \Theta) \cdot \pr (\Theta)}{\pr (\text{data})}.
\end{equation}
Consider a case in which we have a sample of $N$ data points $x_1, \ldots, x_N$. 
We assume that the likelihood is a Poisson distribution with mean $\lambda$ and 
that the prior for $\lambda$ is a log-normal$(1, 1)$ distribution. To calculate 
the probability of the data $\pr (\text{data})$, we must evaluate the integral:
\begin{equation}
	\pr (\text{data}) = \int_{0}^{\infty} 
		\prod_{i = 1}^N \frac{\lambda^{x_i} e^{- \lambda}}{x_i!} \cdot 
		\frac{1}{\sqrt{2 \pi} \lambda} e^{- \frac{1}{2} (\log \lambda - 1)^2} 
		d\lambda.
\end{equation}
While this integral is not too difficult, it explains the problem of calculating 
posteriors analytically. As the number of parameters (the length of $\Theta$) 
increases, calculating the probability of the data requires evaluating integrals 
in higher dimensional spaces. This is why we use alternative methods to derive 
approximate versions of the posterior.

\section{Trees, Boosting and Random Forests}

\section{Neural Networks}

\begin{thebibliography}{99}
\bibitem{BliHwa}  Joseph K.~Blitzstein, Jessica Hwang. \emph{Introduction 
to Probability}, Second Edition, Chapman and Hall, 2019.

\bibitem{BroDav} Peter J.~Brockwell, Richard A.~Davis. 
\emph{Introduction to Time Series and Forecasting}, Third Edition, Springer, 
2016. 

\bibitem{CasBer} George Casella, Roger L.~Berger. \emph{Statistical Inference}, 
Second Edition, Duxbury Advanced Series, 2001.

\bibitem{DegSch} Morris DeGroot, Mark J.~Schervish. \emph{Probability and 
Statistics}, Fourth Edition, Pearson, 2012.
  
\bibitem{Hyn} Rob J.~Hyndman and George Athanasopoulos. \emph{Forecasting: 
Principles and Practice}, Second Edition. Online book at: 
\url{https://otexts.com/fpp2/}

\bibitem{Lam} Ben Lambert. \emph{A Student's Guide to Bayesian Statistics}, 
Sage Publications, 2018. 

\end{thebibliography}
\end{document}
