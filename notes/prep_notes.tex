\documentclass[11pt,a4]{article}
\usepackage{amsmath}
\usepackage{url}
\usepackage[left=2cm,right=2cm,top=3cm,bottom=2cm]{geometry}

\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\sd}{\sigma}
\DeclareMathOperator{\pr}{P}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\var}{Var}
% for the differential operator
\DeclareMathOperator{\der}{d}

\newcommand{\ts}[3]{\ensuremath{\left \{ #1 \right \}_{#2}^{#3}}}
\newcommand{\eqd}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\iid}{\ensuremath{\stackrel{\text{iid}}{\sim}}}
\title{ML Preparation Notes}

\begin{document}
\maketitle

\section{Basics}

The \emph{correlation} between two sets of data is a measure of the strength of the relationship between them. In particular, Pearson's correlation coefficient 
is a measure of linear relationship between two sets of data. Let $X$ and $Y$ 
be two random variables. Then Pearson's correlation coefficient $\rho(X, Y)$ 
is defined as:
\begin{equation}
	\rho(X, Y) = \frac{\cov(X, Y)}{\sd_X \sd_Y} 
\end{equation}  
Two important facts about the Pearson's correlation coefficient~\cite{CasBer}:
\begin{enumerate}
	\item $-1 \leq \rho(X, Y) \leq 1$
	\item $|\rho(X, Y)| = 1$ iff there exists $a \neq 0$ and $b$ such that 
	$Y = aX + b$.
\end{enumerate}


\section{Timeseries}

The material in the section follows~\cite{BroDav}. A \emph{time series} is a 
sequence of random variables $\{X_1, X_2, \ldots \}$. A complete model of such 
a time series would require specifying the joint probability distributions 
of all random vectors $(X_1, \ldots, X_n)'$ for all $n \geq 1$. In practice, 
this might be impossible to do unless the time series is generated by some 
simple well-understood mechanism. Instead one typically specifies the first- and 
second-order moments of the joint distibutions, that is, $\E[X_t]$ and $\E[X_t X_{t + h}]$ for all $t \geq 1$ and for all $h \geq 0$. 

The next important concepts are those of stationarity and the 
auto-correlation function. Roughly speaking, a time series 
$\ts{X_t}{t = -\infty}{\infty}$ is stationary if its ``statistical properties'' are similar to those of the time-shifted series $\ts{X_{t + h}}{t = -\infty}{\infty}$ for every integer ``lag''~$h$. By statistical properties, we mean 
the first- and second-order moments of $\ts{X_t}{}{}$. 

Formally, a time series $\ts{X_t}{}{}$ is weakly stationary if 
\begin{enumerate}
	\item $\E[X_t]$ is independent of $t$
	\item $\cov(X_t, X_{t + h})$ is independent of $t$ for every fixed lag~$h$.
\end{enumerate} 
In contrast, a time series $\ts{X_t}{}{}$ is strictly stationary if the random vectors $(X_{t_1}, \ldots, X_{t_n})'$ and $(X_{t_1 + h}, \ldots, X_{t_n + h})'$
have the same joint distributions for all sets of indices 
$\{t_1, \ldots, t_n\}$, for all $h \geq 0$ and all $n \geq 1$. This is written 
as:
\[
(X_{t_1}, \ldots, X_{t_n})' \eqd (X_{t_1 + h}, \ldots, X_{t_n + h})'
\] 

Strict stationarity implies the following:
\begin{enumerate}
	\item The random variables $X_t$ are identically distributed.
	
	\item Pairs of random variables $(X_t, X_{t + h})$ have the same 
	distribution as $(X_1, X_{1 + h})$ (set $n = 2$).
	
	\item Strict stationarity implies weak stationarity: $\E[X_t] = \E[X_1]$
	and $\cov(X_t, X_{t + h}) = \cov(X_1, X_{1 + h})$ for all $t \geq 1$ and 
	all $h \geq 0$. Both terms are independent of $t$.
	
	\item Weak stationarity does \emph{not} imply strong stationarity. We show
	this by an example. Let $Z_i \iid N(0, 1)$ for all $i$. Define $X_t$ as:
	\[
		X_t = \left \{ \begin{array}{ll}
							Z_t   & \text{ if $t$ is even}\\
							2 Z_t & \text{ if $t$ is odd}.   
					   \end{array}	\right .
	\] 
	Then $\E[X_t] = 0$ for all $t$ and $\cov(X_t, X_{t + h}) = 0$, since $X_t$ 
	and $X_{t + h}$ are independent. However, $X_0$ and $X_1$ do not have the 
	same distribution.
\end{enumerate}

The autocovariance function of a stationary time series $\{ X_t \}$ at lag~$h$
is defined as $\cov(X_{t + h}, X_{t}) = \cov(X_{h}, X_{0})$. The autocorrelation
of $\{ X_t \}$ at lag~$h$ is defined as 
\[
	\frac{ \cov(X_{t + h}, X_t) }{ \var(X_t) } = 
	\frac{ \cov( X_h, X_0 )}{ \var(X_0) }.
\]   

\section{Bayesian Statistics and MCMC}
This section is based on Chapters~12--15 from~\cite{Lam}. Bayes' rule gives us a
 recipe for calculating the posterior probability density. 
\begin{equation}
	\pr (\Theta \mid \text{data}) = 
	\frac{\pr (\text{data} \mid \Theta) \cdot \pr (\Theta)}{\pr (\text{data})}.
\end{equation}
Consider a case in which we have a sample of $N$ data points $x_1, \ldots, x_N$. 
We assume that the likelihood is a Poisson distribution with mean $\lambda$ and that
the prior for $\lambda$ is a log-normal$(1, 1)$ distribution. To calculate the probability
of the data $\pr (\text{data})$, we must evaluate the integral:
\begin{equation}
	\pr (\text{data}) = \int_{0}^{\infty} 
		\prod_{i = 1}^N \frac{\lambda^{x_i} e^{- \lambda}}{x_i!} \cdot 
		\frac{1}{\sqrt{2 \pi} \lambda} e^{- \frac{1}{2} (\log \lambda - 1)^2} 
		d\lambda.
\end{equation}
While this integral is not too difficult, it explains the problem of calculating 
posteriors analytically. As the number of parameters (the length of $\Theta$) 
increases, calculating the probability of the data requires evaluating integrals 
in higher dimensional spaces. This is why we use alternative methods to derive 
approximate versions of the posterior.

\section{Trees, Boosting and Random Forests}

\section{Neural Networks}

\begin{thebibliography}{99}

\bibitem{BroDav} Peter J.~Brockwell, Richard A.~Davis. \emph{Introduction to Time Series and Forecasting}, Third Edition, Springer, 2016. 

\bibitem{CasBer} George Casella, Roger L.~Berger. \emph{Statistical Inference}, 
Second Edition, Duxbury Advanced Series, 2001.

\bibitem{Hyn} Rob J.~Hyndman and George Athanasopoulos. \emph{Forecasting: Principles and Practice}, Second Edition. Online book at: \url{https://otexts.com/fpp2/}

\bibitem{Lam} Ben Lambert. \emph{A Student's Guide to Bayesian Statistics}, Sage 
Publications, 2018. 

\end{thebibliography}
\end{document}
