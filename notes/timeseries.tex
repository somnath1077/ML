\chapter{Timeseries}

The material in the section follows~\cite{BroDav}. A \emph{time series} is a
sequence of random variables $\{X_1, X_2, \ldots \}$. A complete model of such
a time series would require specifying the joint probability distributions of
all random vectors $(X_1, \ldots, X_n)'$ for all $n \geq 1$. In practice, this
might be impossible to do unless the time series is generated by some simple
well-understood mechanism. Instead one typically specifies the first- and
second-order moments of the joint distibutions, that is, $\E[X_t]$ and $\E[X_t
X_{t + h}]$ for all $t \geq 1$ and for all $h \geq 0$. 

The next important concepts are those of stationarity and the auto-correlation
function. Roughly speaking, a time series $\ts{X_t}{t = -\infty}{\infty}$ is
stationary if its ``statistical properties'' are similar to those of the
time-shifted series $\ts{X_{t + h}}{t = -\infty}{\infty}$ for every integer
``lag''~$h$. By statistical properties, we mean the first- and second-order
moments of $\ts{X_t}{}{}$. 

Formally, a time series $\ts{X_t}{}{}$ is weakly stationary if 
\begin{enumerate}
	\item $\E[X_t]$ is independent of $t$
	\item $\cov(X_t, X_{t + h})$ is independent of $t$ for every fixed lag~$h$.
\end{enumerate} 
In contrast, a time series $\ts{X_t}{}{}$ is strictly stationary if the random
vectors $(X_{t_1}, \ldots, X_{t_n})'$ and $(X_{t_1 + h}, \ldots, X_{t_n + h})'$
have the same joint distributions for all sets of indices $\{t_1, \ldots,
t_n\}$, for all $h \geq 0$ and all $n \geq 1$. This is written as:
\[
(X_{t_1}, \ldots, X_{t_n})' \eqd (X_{t_1 + h}, \ldots, X_{t_n + h})'
\] 

Strict stationarity implies the following:
\begin{enumerate}
\item The random variables $X_t$ are identically distributed.
	
	\item Pairs of random variables $(X_t, X_{t + h})$ have the same
distribution as $(X_1, X_{1 + h})$ (set $n = 2$).
	
	\item Strict stationarity implies weak stationarity: $\E[X_t] =
\E[X_1]$ and $\cov(X_t, X_{t + h}) = \cov(X_1, X_{1 + h})$ for all $t \geq 1$
and all $h \geq 0$. Both terms are independent of $t$.
	
	\item Weak stationarity does \emph{not} imply strong stationarity. We
show this by an example. Let $Z_i \iid N(0, 1)$ for all $i$. Define $X_t$ as:
\[ X_t = \left \{ \begin{array}{ll} Z_t   & \text{ if $t$ is even}\\ 
                                    2 Z_t & \text{ if $t$ is odd}.   
                  \end{array}	
         \right . 
\] 
Then $\E[X_t] = 0$ for all $t$. Also, 
\[
    \cov(X_t, X_{t + h}) = \left \{ 
                                \begin{array}{ll}
                                    0 & \text{ if } h > 0 \\
                                    1 & \text{ if } h = 0.
                                \end{array}
                           \right . 
\]
This follows from the assumed independence of $X_t$ and $X_{t + h}$ when $h > 0$. 
However, $X_t$s do not have the same distribution, a requirement of 
strict stationarity.
\end{enumerate}

The autocovariance function of a stationary time series $\{ X_t \}$ at lag~$h$
is defined as $\cov(X_{t + h}, X_{t}) = \cov(X_{h}, X_{0})$. The autocorrelation
of $\{ X_t \}$ at lag~$h$ is defined as 
\[
	\frac{ \cov(X_{t + h}, X_t) }{ \var(X_t) } = 
	\frac{ \cov( X_h, X_0 )}{ \var(X_0) }.
\]   

\begin{example}[iid Noise]
Suppose $\{ X_t \}$ is iid noise with zero mean and 
$\E [X_t^2] = \sigma^2 < \infty$. Then $\E [X_t] = 0$ for all $t$ and for every
fixed lag $h$ and all $t$:
\[\cov(X_t, X_{t + h}) = \left \{ 
                                \begin{array}{ll}
                                    0        & \text{ if } h > 0 \\
                                    \sigma^2 & \text{ if } h = 0.
                                \end{array}
                          \right . 
\]
Thus iid noise is stationary. We denote such a series as 
$\{ X_t \} \sim \IID (0, \sigma^2)$.
\end{example}

\begin{example}[White Noise]
A sequence $\{ X_t \}$ of \emph{uncorrelated} random variables with zero mean 
and finite second moment $\sigma^2$ is called \emph{white noise}. Clearly, the 
covariance function at lag $h$ is the same as that of iid noise and, as such, 
white noise is stationary. We denote such a series as
$\{ X_t \} \sim \WN (0, \sigma^2)$. Unlike iid noise, the 
components of white noise need not be independent (recall that independence 
implies zero correlation but not the other way around). In particular, every
$\IID (0, \sigma^2)$ sequence is a $\WN (0, \sigma^2)$ sequence 
but not the other way around. 
\end{example}

\begin{example}[Random Walk]
 A \emph{random walk} $\{ S_t \}$ is a sequence obtained by cumulatively summing 
iid random variables. A random walk with zero mean is obtained by defining 
$S_0 = 0$ and 
\[
    S_t = X_1 + \cdots + X_t
\]
for all $t > 0$, where $\{ X_t \} \sim \IID(0, \sigma^2)$. In this case, 
$\E [S_t] = 0$ and $\E [S_t^2] = t \sigma^2 < \infty$ for all $t$. For all lags 
$h \geq 0$ and all $t$,
\begin{align*}
    \cov (S_t, S_{t + h}) 
        & = \cov (S_t, S_t + X_{t + 1} + \cdots + X_{t + h}) \\
        & = \cov (S_t, S_t) + \cov (S_t, X_{t + 1}) + \cdots + \cov (S_t, X_{t + h}) \\
        & = t \sigma^2.  
\end{align*}
The last equality follows since $\cov (S_t, X_{t + i}) = 0$ for all $i \geq 1$.
Hence $\cov (S_t, S_{t + h})$ depends on $t$ and $\{ S_t \}$ is not stationary.
\end{example}

\begin{example}[First-Order Moving Average]
Consider the series defined by
\begin{equation}
    X_t = Z_t + \theta Z_{t - 1}, \quad t = 0, \pm 1, \pm 2, \ldots,
\end{equation}
where $Z_t \sim \WN(0, \sigma^2)$ and $\theta$ is a real-valued constant. Now 
$\E [X_t] = 0$ and 
\begin{align*}
    \E [X_t^2] & = \E [ (Z_t + \theta Z_{t - 1})^2 ] \\
               & = \E [ Z_t^2 + 2 \theta Z_t Z_{t - 1} + \theta^2 Z_{t - 1}^2] \\
               & = \sigma^2 + 2 \theta \E [Z_t Z_{t - 1}] +  \theta^2 \sigma^2 \\
               & = (1 + \theta^2) \sigma^2.
\end{align*}
The last equality follows since, $Z_t$ and $Z_{t - 1}$ being uncorrelated, 
satisfy $\cov (Z_t, Z_{t - 1}) = 0$. Recall that 
$\cov (Z_t, Z_{t - 1}) = \E [Z_t Z_{t - 1}] - \E [Z_t] \E [Z_{t - 1}]$ 
and that being uncorrelated is sufficient for the expectation of the product 
of two random variables to be equal to the product of their expectations. One 
can easily verify that for all $t$
\begin{equation}
    \cov (X_t, X_{t + h}) = 
        \left \{ 
            \begin{array}{ll}
                (1 + \theta^2) \sigma^2 & \text{ if } h = 0 \\
                \theta \sigma^2         & \text{ if } h = \pm 1 \\
                0                       & \text{ if } |h| \geq 2.
            \end{array}        
        \right . 
\end{equation} 
Thus the conditions of weak stationarity hold and the sequence $\{ X_t \}$ 
is weakly stationary. The autocorrelation function is given by:
\begin{equation}
    \rho (h) = \frac{\cov (X_0, X_{h})}{\var(X_0)} = 
        \left \{ 
            \begin{array}{ll}
                1                             & \text{ if } h = 0 \\
                \frac{\theta}{1 + \theta^2}   & \text{ if } h = \pm 1 \\
                0                             & \text{ if } |h| \geq 2.
            \end{array}        
        \right . 
\end{equation} 
\end{example}

\begin{example}[First-Order Autoregression]
Let $\{ X_t \}$ be a stationary series satisfying the equation:
\begin{equation}
    X_t = \phi X_{t - 1} + Z_t \quad t = 0, \pm 1, \pm 2, \ldots,
\end{equation}
where $\{ Z_t \} \sim \WN(0, \sigma^2)$, $|\phi| < 1$ and $Z_t$ is uncorrelated 
with $X_s$ for all $s < t$. As can be seen, $\E [X_t] = 0$ and for $h > 0$:
\begin{align*}
\cov (X_{t + h}, X_t) 
    & = \cov (\phi X_{t + h - 1} + Z_{t + h}, X_t) \\
    & = \phi \cov (X_{t + h - 1}, X_t) + \cov (Z_{t + h}, X_t) \\
    & = \phi \cov (X_{t + h - 1}, X_t).
\end{align*}
From this, one can show that $\cov (X_{t + h}, X_t) = \phi^h \cov (X_t, X_t)$. 
By assumption, $\{ X_t \}$ is stationary and hence 
$\cov (X_{t + h}, X_t) = \phi^h \cov (X_0, X_0)$
Next suppose that $h < 0$. Let $s = t + h = t - |h|$ so that $t = s + |h|$. 
Then $\cov (X_{t + h}, X_t) = \cov (X_s, X_{s + |h|}) = 
\phi^{|h|} \cov (X_s, X_s) = \phi^{|h|} \cov (X_0, X_0)$. 
The autocorrelation function at lag $h$ is 
\[
    \rho(h) = \frac{ \phi^{|h|}  \cov (X_0, X_0) }{ \cov (X_0, X_0)} = \phi^{|h|}.
\]

One can also obtain a closed-form expression for $\cov (X_0, X_0)$. Since 
$\cov (X_0, X_0) = \cov (X_t, X_t)$ and 
\[
    \cov (X_t, X_t) = \cov (\phi X_{t - 1} + Z_t, \phi X_{t - 1} + Z_t) 
    = \phi^2 \cov (X_0, X_0) + \cov (Z_t, Z_t), 
\]
we obtain that $\cov (X_0, X_0) = \sigma^2 / (1 - \phi^2)$.
\end{example}



